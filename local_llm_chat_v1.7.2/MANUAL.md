# Local LLM Chat v1.7.2 マニュアル

**2026 JRC ハンズオンセミナー 配布資料**

**作成責任者**: 和田昭彦

---

## 概要

Local LLM Chatは、ローカルで動作するLLMサーバー（LM Studio、Ollamaなど）と連携するWebベースのチャットインターフェースです。完全オフラインで動作し、プライバシーを重視した設計になっています。

**v1.7.2 新機能:**
- 🏥 **医学用語チェック**: 送信前に不正確な医学用語をLLMでチェック

**v1.7.1 機能:**
- 📊 **モデル比較機能**: 2つのモデルの回答を並べて比較表示（目玉機能）
- **モデルリスト自動更新**: ドロップダウンクリック時に自動更新（Refreshボタン廃止）
- **ヘッダーUI改善**: ボタンを機能グループごとに整理

---

## 動作要件

- **ブラウザ**: Chrome、Firefox、Safari、Edge（モダンブラウザ）
- **LLMサーバー**: LM Studio（v0.4.0以降推奨）または Ollama
- **デフォルトURL**: `http://localhost:1234/v1`

**v1.7.2の一部機能は LM Studio v0.4.0以降が必要です:**
- モデル比較機能（複数モデルの同時ロード）

---

## LM Studio バージョン別の違い（0.3.x → 0.4.x）

> **ハンズオンセミナー参加者へ**:
> 公式サイトからダウンロードすると **v0.4.1** がインストールされます。
> このセクションは参考情報です。**ハンズオンでは v0.4.1 のみを使用**します。

LM Studio 0.4.x は 0.3.x から大幅にアーキテクチャが刷新されました。以下に主な変更点をまとめます。

### 1. アーキテクチャ：GUI依存から「コア分離」へ

#### llmster（daemon / headless deployment）
- LM Studioのコアが **GUIから切り離されてデーモンとして動作**する設計に再構築
- **サーバー/CI/クラウド等の非GUI環境**でも同じ基盤を使用可能
- 研究室内の閉域サーバーで「GUIなし常駐」が可能に

### 2. 推論性能：並列処理と連続バッチ

#### Parallel requests + Continuous batching
- 0.3系の「キューに並べて順番待ち」から、**同一モデルに対して複数リクエストを同時処理**する設計へ
- `n_parallel`（並列スロット）を使ったチューニングが中心に
- CLIでも `lms load --parallel <N>` が追加され、運用の自動化がしやすく
- **複数ユーザー・複数ワークフローが同時アクセスする構成で体感差が大きい**

### 3. API：ネイティブREST APIの正式リリース

#### LM Studio native v1 REST API（`/api/v1/*`）
0.4.0で **ネイティブv1 REST API** が「公式リリース」として明記されました：

| エンドポイント | 機能 |
|---------------|------|
| `/api/v1/models` | 全ダウンロード済みモデル取得（state付き） |
| `/api/v1/models/load` | モデルロード |
| Stateful chats | ステートフル対話 |
| Authentication | APIトークン設定 |
| MCP via API | ローカルMCPをAPI経由で扱う |

> 0.3系は「OpenAI互換API」でつなぐ用途が中心でしたが、0.4以降は **「LM Studioを運用するための管理API」** が強化されています。

### 4. 互換性：Anthropic互換エンドポイント追加（0.4.1）

#### Anthropic-compatible endpoint
- 0.4.1で **Anthropic互換（Claude系）** の `POST /v1/messages` が追加
- 「OpenAI互換」だけでなく、**複数プロトコル互換**に拡張する方向

### 5. UI/UX：Developer Mode・Split View

- 0.4.0で **UIを刷新**し、Split View（チャット2画面）や Developer Mode などが追加
- モデルロード時に**自動的にAPIサーバーが有効**になる（0.3.xでは手動でサーバー起動が必要だった）

### 6. 安定性：0.4.1での修正

0.4.1は「0.4.0の初期不具合修正」が中心：
- Macでのモデルロード関連のバグ修正（bf16系）
- `lms chat` の非ASCII（日本語など）問題修正
- UIクラッシュやトークンカウントの誤報などの修正

### バージョン比較まとめ

| 機能 | 0.3.x | 0.4.x |
|------|-------|-------|
| サーバー起動 | 手動（開発者タブ） | **モデルロード時に自動** |
| 並列リクエスト | 順番待ち | **同時処理対応** |
| 複数モデル同時ロード | 非対応 | **対応** |
| ネイティブREST API | 非公式 | **正式リリース** |
| Anthropic互換API | なし | **0.4.1で追加** |
| headless運用 | 困難 | **llmsterで対応** |

> **推奨**: Local LLM Chat v1.7.2 の全機能を使用するには **LM Studio v0.4.0以降** を使用してください。

---

## 事前準備：LM Studioでモデルをダウンロードする

Local LLM Chatを使用する前に、LM Studioでモデルをダウンロードしておく必要があります。

**重要**: Local LLM Chatアプリ自体にはモデルのダウンロード機能はありません。モデルの追加・管理はすべてLM Studio側で行います。

### モデルのダウンロード手順

1. **LM Studioを起動**
2. **左側メニューから「探索」を開く**（⌘⇧M）
   - 「Mission Control」ウィンドウが開きます
3. **モデルを検索**
   - 上部の検索バー（「Hugging Faceでモデルを検索...」）にモデル名を入力
   - 例: `qwen`, `llama`, `gemma`, `glm` など
   - 「Staff picks」タブでおすすめモデルも確認できます
4. **フォーマットを選択**（検索バー右側のチェックボックス）
   - **GGUF**: CPU/GPU汎用フォーマット（推奨）
   - **MLX**: Apple Silicon (M1/M2/M3/M4) 最適化フォーマット
   - Windowsは GGUF を選択してください
5. **ダウンロードしたいモデルをリストから選択**
   - 右側にモデルの詳細情報が表示されます（パラメータ数、アーキテクチャなど）
   - 「Download Options」で量子化レベル（4BIT等）とファイルサイズを確認
6. **右下の「ダウンロード」ボタンをクリック**
   - ダウンロード完了を待ちます
   - モデルサイズによっては数分〜数十分かかります

### モデル選択のポイント

| 用途 | 推奨モデルサイズ | 備考 |
|------|----------------|------|
| 一般的な会話・質問応答 | 7B〜14B | バランスが良い |
| 高度な推論・コード生成 | 30B以上 | 高性能だがメモリ消費大 |
| 画像認識（Vision） | 30B以上推奨 | 👁️マーク付きモデルを選択 |
| 軽量・高速応答 | 3B〜7B | メモリが少ない環境向け |

### 量子化レベルについて

モデル名に含まれる `Q4_K_M`, `Q8_0` などは量子化レベルを示します：
- **Q8**: 高品質、メモリ消費大
- **Q6_K**: 品質とサイズのバランス良好
- **Q4_K_M**: 一般的な選択、品質と効率のバランス
- **Q4_K_S**: より軽量、やや品質低下
- **Q3_K**: 軽量だが品質低下あり

迷った場合は **Q4_K_M** を選ぶのがおすすめです。

---

## 起動方法（v0.4.1前提）

> **ポイント**: LM Studio 0.4.1 では「モデルをロード＝使える」状態になります。
> サーバー設定などは**触る必要がありません**。

### 3ステップで開始

1. **LM Studioを起動し、モデルをロード**
   - 画面上部の「読み込むモデルを選択」からモデルを選択
   - ロードが完了すると、**自動的にAPIが有効**になります（`http://localhost:1234`）

2. **`local_llm_chat_v1.7.2.html` をブラウザで開く**

3. **モデルが自動的にドロップダウンに表示される** → 会話開始！

### 疎通確認（うまくいかない場合）

ターミナルで以下を実行：
```bash
curl http://localhost:1234/v1/models
```
- **返答あり** → OK、接続できています
- **返答なし** → LM Studioでモデルがロードされていない可能性

---

## 新しいモデルの追加方法

新しいモデルを追加するには、LM Studioでダウンロードします。

### ダウンロード手順

1. **左側メニューから「探索」を開く**（⌘⇧M）
   - 「Mission Control」ウィンドウが開きます
2. **モデルを検索**
   - 上部の検索バー（「Hugging Faceでモデルを検索...」）にモデル名を入力
   - 例: `qwen`, `llama`, `gemma`, `glm` など
3. **フォーマットを選択**（検索バー右側のチェックボックス）
   - **GGUF**: CPU/GPU汎用フォーマット（推奨）
   - **MLX**: Apple Silicon最適化フォーマット
   - Windowsは GGUF を選択してください
4. **ダウンロードしたいモデルをリストから選択**
   - 右側で詳細情報と量子化レベル（4BIT等）を確認
5. **右下の「ダウンロード」ボタンをクリック**

**💡 モデルの切り替え**: ダウンロード済みのモデルは、Local LLM Chatのモデル選択ドロップダウンから選択すれば自動で読み込まれます。

---

## 画面構成

### ヘッダー
- **タイトル**: Local LLM Chat v1.7.2
- **モデル選択**: ロードされているモデルを選択（👁️マークはVision対応）
  - クリック時に自動でモデル一覧を更新
- **📊 比較**: モデル比較モードのON/OFF（v1.7.0新機能）
- **比較モデル選択**: 比較モードON時のみ表示、2つ目のモデルを選択
- **🆕 新しい話題**: 新しい話題を開始（AIへの履歴送信をリセット、画面は保持）
- **🗑️**: 会話履歴をクリア
- **💾**: 会話履歴をJSONでエクスポート
- **📥**: 会話履歴をJSONからインポート
- **❓**: ヘルプモードのON/OFF（アプリの使い方をLLMに質問）
- **⚙️**: 設定パネルを開く

### 入力エリア（画面下部）
- **📷 Image**: 画像ファイルを添付（複数可）
- **📎 File**: テキスト/PDFファイルを添付（複数可）
- **🔍 深掘り**: 深掘りモードのON/OFF
- **テキスト入力欄**: メッセージを入力
- **🚀 Send**: メッセージを送信
- **⏹ Stop**: 生成を中断
- **📋 Preset**: プリセットプロンプトを挿入

---

## 主な機能

### 1. モデル比較機能（v1.7.0新機能）

📊 比較ボタンで2つのモデルの回答を並べて比較できます。

**使い方:**
1. 📊 比較ボタンをクリックして比較モードをONにする
2. メインモデル（左側）と比較モデル（右側）を選択
3. 質問を入力して送信
4. 2つのモデルの回答がサイドバイサイドでリアルタイム表示される

**特徴:**
- 同じ質問を2つのモデルに同時に送信
- 並列ストリーミングで両方の回答をリアルタイム表示
- モバイル対応（768px以下で縦並び表示）
- 履歴保存はメインモデルの応答のみ（比較は一時的な参照用）

**要件:**
- LM Studio v0.4.0以降（複数モデルの同時ロードが必要）
- 比較したい2つのモデルを事前にLM Studioでロードしておく

### 2. チャット機能
- **ストリーミング応答**: リアルタイムでAIの回答を表示
- **Markdown対応**: コードブロック、リスト、表などを整形表示
- **メッセージ操作**:
  - 📋 Copy: メッセージをコピー
  - 🗑 Delete: メッセージを削除
  - ✏️ Edit: ユーザーメッセージを編集・再送信
  - 🔄 Regenerate: AI応答を再生成

### 3. 画像・ファイル添付
- **画像添付**: Vision対応モデルで画像を認識
  - 📷 Imageボタン、ペースト（Ctrl+V）、ドラッグ＆ドロップ対応
  - 複数画像の同時添付可能
  - サムネイルプレビュー表示
  - **サイズ制限: 20MB以下**
- **ファイル添付**: テキストファイルやPDFの内容を送信
  - 対応形式: .txt, .md, .json, .csv, .xml, .html, .css, .js, .py, .pdf など
  - PDF: テキスト抽出してLLMに送信
  - **サイズ制限: テキストファイル2MB以下、PDF10MB以下**

### 4. Vision対応モデル表示
- ドロップダウンにVision対応モデルは👁️アイコン付きで表示
- 対応キーワード: vision, llava, gemma-3, pixtral, devstral, magistral, qwen-vl, qwen2-vl, qwen3-vl, bakllava, obsidian, moondream, minicpm-v, cogvlm, glm-4v, internlm-xcomposer など

### 5. 深掘りモード
🔍深掘りボタンで有効化すると、より深く分析した回答を促します：
- 根本的な原因や背景の分析
- 異なる視点や解釈の可能性
- 関連する概念や理論との繋がり
- 潜在的な問題点や限界
- 実践的な応用や次のステップ

※ 深掘りモードはページをリロードするとOFFに戻ります。

### 6. 新しい話題
🆕 新しい話題ボタンをクリックすると：
- AIへの会話履歴送信がリセットされます
- 画面上の会話は保持され、区切り線が表示されます
- 話題を変えた際に前の回答が繰り返されることを防ぎます

※ 🗑️ Clearとの違い：Clearは画面も含めてすべて削除しますが、新しい話題は画面上の会話を保持したまま、AIの文脈のみをリセットします。

### 7. ヘルプモード
❓ ボタンで有効化すると、このアプリの使い方をLLMに質問できます：
- アプリの機能や操作方法について質問
- トラブルシューティングのヘルプ
- 設定項目の説明

ヘルプモードON時は、アプリのマニュアル内容がシステムプロンプトに含まれ、LLMがマニュアルを参照して回答します。

※ ヘルプモードはページをリロードするとOFFに戻ります。

### 8. 信頼度・代替候補表示
Settingsパネルで「📊 信頼度・代替候補を表示」をONにすると、AIの応答に以下の情報が追加表示されます：

#### 信頼度（Confidence）
- AIの回答がどれだけ確信を持っているかをパーセンテージで表示
- **高（80%以上）**: 緑色 - AIが自信を持って回答
- **中（50-80%）**: 黄色 - ある程度の確信がある
- **低（50%未満）**: 赤色 - 不確かな回答の可能性

#### 代替候補（Alternatives）
- AIが各単語を選ぶ際に検討した他の候補を表示
- 例: 「肺炎」を選んだ箇所で「気管支炎(15%)」「COPD(7%)」なども検討していた場合に表示
- 鑑別診断や複数の解釈がある場合に有用

**注意事項:**
- この機能には **LM Studio v0.3.39以降** が必要です
- Open Responses API（logprobs機能）を使用しています
- 古いバージョンのLM Studioでは情報が表示されない場合があります

### 9. スマートスクロール
ストリーミング応答中のスクロール操作が可能になりました：
- **上にスクロール**: 自動スクロールが一時停止し、過去のメッセージを確認可能
- **下部に戻る**: 自動スクロールが再開

### 10. プリセットプロンプト
📋 Presetボタンで定型プロンプトを挿入：
- 🏥 疾患解説
- 💊 鑑別診断
- 📄 文章要約
- 📝 論文査読
- 📈 統計解析
- ✉️ 英文メール作成

Settingsでプリセットの編集・追加も可能です。

---

## 設定項目

### 基本設定
| 項目 | 説明 | デフォルト |
|------|------|-----------|
| 🌙 Dark Mode | ダークモードの切替 | OFF |
| 🔗 Base URL | LLMサーバーのURL | http://localhost:1234/v1 |
| 🔑 API Key | 認証キー | lmstudio |
| Temperature | 創造性（0=安定、2=創造的） | 0.7 |
| Max Tokens | 最大出力トークン数 | 2048 |
| 送信キー | メッセージ送信のキー設定 | Enter で送信 |

### 送信キー設定
| 設定 | Enter | Shift+Enter | Ctrl/Cmd+Enter |
|------|-------|-------------|----------------|
| Enter で送信 | 送信 | 改行 | - |
| Ctrl+Enter で送信 | 改行 | 改行 | 送信 |

### 応答スタイル
| スタイル | 説明 |
|----------|------|
| 簡潔 | 要点のみを短く |
| 標準 | バランスの取れた回答 |
| 詳細 | 背景や具体例を含む |
| 専門的 | 技術的詳細重視 |

### ユーザープロフィール
| 項目 | 説明 |
|------|------|
| 専門レベル | 初心者〜専門家 |
| 職業/専門分野 | 例: 放射線科医、学生 |
| 興味・関心 | 例: CT画像診断、機械学習 |

### データ管理
- **🔄 設定をデフォルトに戻す**: 設定のみリセット
- **🗑 すべての保存データを消す**: 履歴・設定・プリセットを全削除

---

## キーボードショートカット

| ショートカット | 動作 |
|---------------|------|
| Enter | メッセージ送信（設定による） |
| Ctrl/Cmd + Enter | メッセージ送信（設定による） |
| Shift + Enter | 改行 |
| Ctrl/Cmd + V | 画像をペースト |
| Ctrl/Cmd + K | 履歴クリア |
| Esc | 設定パネル/プリセットパネルを閉じる |

※ 送信キーは Settings で「Enter で送信」または「Ctrl+Enter で送信」に切り替え可能

---

## Apple Silicon Mac メモリ別運用ガイド

Apple Silicon MacはCPU/GPUが同じメモリ（Unified Memory）を共有するため、メモリ管理が重要です。

### メモリの実際の使用可能量

| 搭載RAM | システム予約 | GPU上限(≈75%) | **実質使用可能** |
|---------|-------------|---------------|-----------------|
| **16GB** | 4-6GB | 12GB | **10-12GB** |
| **24GB** | 4-6GB | 18GB | **16-18GB** |

> **重要**: 70%を超えるとswap発生でパフォーマンスが低下します。

### 16GB Mac の運用指針

#### 推奨モデルサイズ

| モデル | 量子化 | サイズ | 推奨度 |
|--------|--------|--------|--------|
| **7B / 8B** | Q4_K_M | 4-5GB | ⭐⭐⭐ 最適 |
| 7B / 8B | Q5_K_M | 5-6GB | ⭐⭐ 良好 |
| 7B / 8B | Q8_0 | 8-9GB | ⚠️ ギリギリ |
| **3B / 4B** | Q4_K_M | 2-3GB | ⭐⭐⭐ 軽快 |
| 13B / 14B | Q4_K_M | 8-9GB | ❌ 非推奨 |

#### 推奨設定

- **context length**: 2048〜4096（最大8192）
- **parallel**: 1（固定、触らない）
- **KV cache量子化**: 有効推奨

#### 安全な組み合わせ

| モデル | context | 状態 |
|--------|---------|------|
| 7B Q4 | 4096 | ✅ 安全 |
| 7B Q4 | 8192 | ⚠️ 注意 |
| 3B Q4 | 8192 | ✅ 可能 |

#### 16GB Mac での注意事項

- **ブラウザ・他アプリは閉じる**（メモリ確保）
- **長い会話は「新しい話題」でリセット**
- **複数モデル同時ロードは避ける**

### 24GB Mac の運用指針

#### 推奨モデルサイズ

| モデル | 量子化 | サイズ | 推奨度 |
|--------|--------|--------|--------|
| **7B / 8B** | Q4_K_M | 4-5GB | ⭐⭐⭐ 余裕 |
| 7B / 8B | Q8_0 | 8-9GB | ⭐⭐⭐ 快適 |
| **13B / 14B** | Q4_K_M | 8-9GB | ⭐⭐ 良好 |
| 13B / 14B | Q5_K_M | 10-11GB | ⚠️ 注意 |
| 27B | Q4_K_M | 15-16GB | ⚠️ ギリギリ |
| 70B | 任意 | 35GB+ | ❌ 不可 |

#### 推奨設定

- **context length**: 4096〜8192（最大16384）
- **parallel**: 1〜2
- **KV cache量子化**: 有効推奨

#### 安全な組み合わせ

| モデル | context | parallel | 状態 |
|--------|---------|----------|------|
| 7B Q4 | 8192 | 2 | ✅ 安全 |
| 8B Q8 | 4096 | 2 | ✅ 安全 |
| 13B Q4 | 4096 | 1 | ✅ 安全 |
| 13B Q4 | 8192 | 1 | ⚠️ 注意 |
| 27B Q4 | 4096 | 1 | ⚠️ 限界 |

### メモリ不足のサイン

以下の症状が出たらメモリ不足です：

- レスポンスが極端に遅くなる（数秒→数十秒）
- Mac全体が重くなる
- ファンが回り始める

#### 対処法

1. **モデルをアンロード**（LM Studioでモデル名をクリック → Unload）
2. 他のアプリを終了
3. より小さいモデル or 短いcontextで再試行

### ハンズオン向け推奨モデル（MLX 4-bit）

> **MLXとは**: Apple Silicon向けに最適化されたフォーマット。GGUFより高速で効率的です。
> LM Studioでモデル検索時に「MLX」にチェックを入れてダウンロードしてください。

#### 16GB Mac

| モデル | サイズ | 特徴 | 推奨度 |
|--------|--------|------|--------|
| **Gemma 3 4B** (4-bit) | 約2.6GB | 軽量・高速、Vision対応 👁️ | ⭐⭐⭐ |
| **Qwen 3 4B** (4-bit) | 約2.5GB | 軽量・日本語良好 | ⭐⭐⭐ |
| Qwen 3 8B (4-bit) | 約4.3GB | バランス良好 | ⭐⭐ |
| Gemma 3 12B (4-bit) | 約6.6GB | 高品質、メモリ余裕少 | ⚠️ |

**推奨**: `gemma-3-4b-it-4bit` または `Qwen3-4B-4bit`（context: 2048〜4096）

#### 24GB Mac

| モデル | サイズ | 特徴 | 推奨度 |
|--------|--------|------|--------|
| **Gemma 3 12B** (4-bit) | 約6.6GB | 高品質、Vision対応 👁️ | ⭐⭐⭐ |
| **Qwen 3 8B** (4-bit) | 約4.3GB | 日本語優秀、高速 | ⭐⭐⭐ |
| **gpt-oss 20B** (4-bit) | 約11GB | OpenAI公式、MoE高効率 | ⭐⭐⭐ |
| Qwen 3 14B (4-bit) | 約8-9GB | 高性能 | ⭐⭐ |
| Gemma 3 27B (4-bit) | 約15GB | 最高品質、メモリ限界 | ⚠️ |

**推奨**: `gemma-3-12b-it-4bit`、`Qwen3-8B-4bit`、または `gpt-oss-20b-4bit`（context: 4096〜8192）

> **gpt-oss 20Bとは**: OpenAI初のオープンソースモデル（Apache 2.0）。21Bパラメータ中3.6Bのみ活性化するMoE構造で、メモリ効率が非常に高い。

### モデル選択ガイド

| 用途 | 16GB Mac | 24GB Mac |
|------|----------|----------|
| **画像認識（Vision）** | Gemma 3 4B 👁️ | Gemma 3 12B 👁️ |
| **日本語重視** | Qwen 3 4B / 8B | Qwen 3 8B / 14B |
| **軽量・高速** | Gemma 3 4B | Qwen 3 8B |
| **高品質回答** | Qwen 3 8B | gpt-oss 20B / Gemma 3 12B |
| **OpenAI互換** | - | gpt-oss 20B |

### LM Studioでのダウンロード方法

1. LM Studio → 探索（⌘⇧M）
2. 検索バー右側で **「MLX」にチェック** ✓
3. `gemma 3` または `qwen 3` で検索
4. `4bit` バージョンを選択してダウンロード

### まとめ

| RAM | 推奨モデル（MLX 4-bit） | context上限 |
|-----|------------------------|------------|
| **16GB** | Gemma 3 4B / Qwen 3 4B | 4096 |
| **24GB** | Gemma 3 12B / Qwen 3 8B / gpt-oss 20B | 8192 |

---

## トラブルシューティング

### まず確認：curlで疎通テスト

問題が発生したら、まずターミナルで以下を実行：
```bash
curl http://localhost:1234/v1/models
```
- **返答あり** → LM Studioは正常
- **返答なし** → LM Studioでモデルがロードされていない

### モデルが表示されない
1. **モデルがロードされているか確認**（LM Studio画面上部でモデル名が表示されているか）
2. 上記curlコマンドで疎通確認
3. モデル選択ドロップダウンをクリックして更新

### 比較機能が動作しない
1. **2つのモデルがLM Studioでロードされているか確認**
2. 比較モード（📊ボタン）がONになっているか確認

### 画像が認識されない
1. Vision対応モデル（👁️マーク付き）を選択しているか確認
2. 画像サイズが20MB以下か確認
3. モデルサイズは30B以上を推奨

### 応答が遅い・途切れる
1. Max Tokensを減らす
2. より小さいモデルに変更
3. PCのメモリ・GPU使用状況を確認

### 接続エラーが発生する
1. **モデルがロードされているか確認**（最重要）
2. Base URLが正しいか確認（デフォルト: `http://localhost:1234/v1`）

### 応答が途中で消える
1. Settings → 「🗑 すべての保存データを消す」で履歴をクリア
2. ページを強制リロード（Cmd+Shift+R または Ctrl+Shift+R）

---

## データ保存について

すべてのデータはブラウザのlocalStorageに保存されます：
- `localLLMChat_history`: 会話履歴
- `localLLMChat_settings`: 設定
- `localLLMChat_presets`: カスタムプリセットの内容
- `localLLMChat_presetLabels`: カスタムプリセットのラベル
- `localLLMChat_draft`: 入力中の下書き（自動保存）

データはブラウザを閉じても保持されます。プライベートモード/シークレットモードでは保存されません。

### コンテキスト管理
APIには最後の6メッセージ（システムプロンプト含む）のみ送信されます。これはモデルのコンテキスト長を効率的に使用するための仕様です。会話履歴自体はすべてlocalStorageに保存されます。

---

## 技術情報

- **対応API**: OpenAI互換API（LM Studio、Ollama、LocalAI など）
- **外部ライブラリ**: marked.js（Markdown）、PDF.js（PDF抽出）
- **オフライン対応**: すべてのライブラリはローカルに同梱

---

## バージョン履歴

### v1.7.0 (2026-02-01)
- **新機能**
  - 📊 **モデル比較機能**: 2つのモデルの回答を並べて比較表示
  - **モデルリスト自動更新**: ドロップダウンクリック時に自動更新
- **UI改善**
  - ヘッダーボタンを機能グループごとに整理（縦線で区切り）
  - Export/Import/Clearをアイコンのみに
  - モデルリストをアルファベット順（表示名）でソート
- **要件**: 比較機能にはLM Studio v0.4.0以降が必要

### v1.6.8 (2026-01-21)
- **新機能**
  - 🆕 **新しい話題ボタン**: 話題を変えた際に前の回答が混ざる問題を解決
- **UI改善**
  - ❓/⚙️ボタンをアイコンのみに変更（ヘッダー幅削減）
  - 鑑別診断プリセットの内容更新

### v1.6.7 (2026-01-20)
- **新機能**
  - **信頼度表示**: AIの回答の確信度をログ確率から計算して表示
  - **代替候補表示**: AIが検討した他のトークン候補を表示
  - **会話履歴インポート**: JSONファイルから会話履歴を復元（📥 Importボタン）
  - LM Studio v0.3.39以降の Open Responses API（logprobs）に対応
- **設定追加**
  - 「📊 信頼度・代替候補を表示」トグルを追加
- **バグ修正**
  - ストリーミングエラー後のRegenerate（🔄）が機能しない問題を修正

### v1.6.6 (2026-01-19)
- **新機能**
  - ヘルプモード: ❓ Helpボタンでアプリの使い方をLLMに質問できる機能を追加
- **バグ修正**
  - 会話中のLLMモデル切り替えが正しく反映されない問題を修正

### v1.6.5 (2026-01-17)
- **バグ修正**
  - 長いファイル名の添付時に入力エリアが拡大する問題を修正
- **改善**
  - ファイルサイズ制限の調整（テキスト: 2MB、PDF: 10MB）
  - マニュアルの改善・詳細化

### v1.6.4 (2025-12-23)
- **バグ修正**
  - Ctrl+Enter送信設定が効かない問題を修正
  - Stopボタンが押せない問題を修正
- **新機能**
  - スマートスクロール: ストリーミング中のスクロール操作が可能に

詳細は `CHANGELOG.md` を参照してください。

---

## 謝辞

テスト運用とフィードバックにご協力いただいた皆様に感謝いたします。

- 入江隆介先生
- 守田裕一先生
- 笹山大樹先生

---

## 免責事項

本ソフトウェア（Local LLM Chat）は「現状有姿」で提供されます。作成者は、本ソフトウェアの使用または使用不能から生じるいかなる損害（直接的、間接的、偶発的、特別、結果的損害を含むがこれに限定されない）についても、一切の責任を負いません。

本ソフトウェアを使用することにより、ユーザーは以下の事項に同意したものとみなされます：

1. **自己責任での使用**: 本ソフトウェアの使用は、すべてユーザー自身の責任において行われます。
2. **無保証**: 作成者は、本ソフトウェアの正確性、信頼性、完全性、有用性について、いかなる保証も行いません。
3. **AI出力の検証**: AIによる出力結果は参考情報であり、重要な判断を行う前には必ず専門家への相談や独自の検証を行ってください。
4. **医療・法律・財務への非適用**: 本ソフトウェアは医療診断、法的助言、財務アドバイスを提供するものではありません。これらの分野での判断には、必ず資格を持つ専門家にご相談ください。

本ソフトウェアを使用した時点で、上記の免責事項に同意したものとみなされます。
