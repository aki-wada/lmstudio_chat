# Local LLM Chat

ローカルで動作する LLM サーバー（LM Studio、Ollama など）と連携する Web ベースのチャットインターフェース

## 概要

Local LLM Chat は、ローカル環境で動作する LLM サーバーの AI モデルとインタラクティブに対話できる、シンプルで使いやすいチャットアプリケーションです。単一の HTML ファイルで構成されており、セットアップが簡単で、ブラウザさえあればすぐに使い始めることができます。OpenAI 互換 API をサポートするすべてのローカル LLM サーバーで利用可能です。

## 主な機能

- ✨ **自動モデル同期**: LM Studio で利用可能なモデルを自動検出
- 🔄 **ストリーミング応答**: リアルタイムで AI の応答を表示
- 🖼️ **Vision 対応**: 画像入力とマルチモーダル対話に対応
- ⚙️ **カスタマイズ可能な設定**: Temperature、Max Tokens、System Prompt などを調整可能
- 💬 **応答スタイル**: 簡潔/標準/詳細/専門的の 4 つから選択
- 👤 **ユーザープロフィール**: 専門レベル、職業、興味に合わせた応答
- 🌙 **ダークモード**: 目に優しいダークテーマ
- 📋 **メッセージ管理**: コピー、削除、再生成機能
- 💾 **履歴保存**: ブラウザに会話履歴を自動保存
- 📤 **エクスポート**: 会話履歴を JSON 形式でダウンロード

## セットアップ

### 必要な環境

- **LM Studio**: ローカルにインストールされ、サーバーが起動していること
- **Web ブラウザ**: Chrome、Firefox、Safari、Edge など

### インストール手順

1. LM Studio（または他の OpenAI 互換 LLM サーバー）をインストールして起動
2. 使用したいモデルをロード
3. サーバーを起動（デフォルト: `http://localhost:1234`）
4. `local_llm_chat.html` をブラウザで開く

### 使い方

詳しい使い方は [MANUAL.md](MANUAL.md) を参照してください。

## ファイル構成

```
lmstudio_chat/
├── local_llm_chat.html   # メインアプリケーション
├── MANUAL.md             # ユーザーマニュアル
├── README.md             # このファイル
├── CHANGELOG.md          # 変更履歴
└── LICENSE               # ライセンス情報
```

## 技術仕様

- **フロントエンド**: HTML5, CSS3, JavaScript (Vanilla JS)
- **API**: OpenAI 互換 API（LM Studio、Ollama、llama.cpp など）
- **ストレージ**: localStorage (ブラウザ)
- **外部依存**: marked.js (Markdown レンダリング)

## バージョン情報

- **現在のバージョン**: 1.1.0
- **最終更新**: 2025-11-29

## 開発者向け情報

### 主要な機能実装

- **応答スタイルカスタマイズ**: システムプロンプトに応答スタイルを動的に追加
- **ユーザープロフィール**: 専門レベルに応じた説明スタイルの調整
- **Vision API**: base64 エンコードした画像をマルチモーダルメッセージとして送信
- **ストリーミング**: Server-Sent Events (SSE) を使用したリアルタイム応答

### カスタマイズ

システムプロンプト、応答スタイル、ユーザープロフィールは Settings パネルから簡単に変更できます。

## ライセンス

MIT License - 詳細は [LICENSE](LICENSE) を参照

## サポート

問題が発生した場合は、まず [MANUAL.md](MANUAL.md) のトラブルシューティングセクションを確認してください。

## 貢献

バグ報告や機能提案は大歓迎です。

---

**Happy Chatting! 🎉**
