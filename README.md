# Local LLM Chat

ローカルで動作する LLM サーバー（LM Studio、Ollama など）と連携する Web ベースのチャットインターフェース

## 概要

Local LLM Chat は、ローカル環境で動作する LLM サーバーの AI モデルとインタラクティブに対話できる、シンプルで使いやすいチャットアプリケーションです。完全オフラインで動作し、プライバシーを重視した設計になっています。

## 最新版: v1.7.0

**2026 JRC ハンズオンセミナー 配布版**

### v1.7.0 新機能

- 📊 **モデル比較機能**: 2つのモデルの回答を並べて比較表示（目玉機能）
- **モデルリスト自動更新**: ドロップダウンクリック時に自動更新
- **ヘッダーUI改善**: ボタンを機能グループごとに整理

### 主な機能

- ✨ **自動モデル検出**: LM Studio でロードされているモデルを自動検出
- 🔄 **ストリーミング応答**: リアルタイムで AI の応答を表示
- 📊 **モデル比較**: 2つのモデルの回答をサイドバイサイドで比較（v1.7.0新機能）
- 🖼️ **Vision 対応**: 画像入力とマルチモーダル対話に対応（👁️マークで識別）
- 📎 **ファイル添付**: テキストファイル、PDF、画像を添付して送信可能
- 📋 **プリセットプロンプト**: 医療・研究向けテンプレートを素早く挿入
- ⚙️ **カスタマイズ可能な設定**: Temperature、Max Tokens、System Prompt などを調整可能
- 💬 **応答スタイル**: 簡潔/標準/詳細/専門的の 4 つから選択
- 🌙 **ダークモード**: 目に優しいダークテーマ
- 💾 **履歴保存**: ブラウザに会話履歴を自動保存
- 📤 **エクスポート/インポート**: 会話履歴を JSON 形式で保存・復元

## 動作要件

- **LM Studio**: v0.4.0 以降推奨（比較機能には必須）
- **Web ブラウザ**: Chrome、Firefox、Safari、Edge（モダンブラウザ）
- **デフォルトURL**: `http://localhost:1234/v1`

## クイックスタート（3ステップ）

> **ポイント**: LM Studio v0.4.1 では「モデルをロード＝使える」状態になります。
> サーバー設定などは触る必要がありません。

### 1. LM Studio でモデルをロード

- LM Studio を起動
- 画面上部の「読み込むモデルを選択」からモデルを選択
- ロードが完了すると、**自動的にAPIが有効**になります

### 2. HTML ファイルを開く

- `local_llm_chat_v1.7.0.html` をブラウザで開く

### 3. 会話開始！

- モデルが自動的にドロップダウンに表示される
- メッセージを入力して送信

### 疎通確認（うまくいかない場合）

ターミナルで以下を実行：
```bash
curl http://localhost:1234/v1/models
```
- **返答あり** → OK
- **返答なし** → LM Studio でモデルがロードされていない

### CORS 設定（接続できない場合）

LM Studio 設定で「**CORSを有効にする**」をONにしてください：
- 設定（⚙️）→「Local Server」セクション →「Enable CORS」をON

## ファイル構成

### v1.7.0（最新版・推奨）

```
local_llm_chat_v1.7.0/
├── local_llm_chat_v1.7.0.html   # メインアプリケーション
├── MANUAL.md                     # ユーザーマニュアル
├── MANUAL_print.html             # 印刷用マニュアル
├── assets/                       # 外部ライブラリ（オフライン対応）
│   ├── app.css                   # スタイルシート
│   ├── marked.min.js             # Markdown レンダリング
│   ├── pdf.min.js                # PDF テキスト抽出
│   └── pdf.worker.min.js         # PDF.js Worker
└── js/
    └── app.js                    # メイン JavaScript
```

### 配布ファイル

- `local_llm_chat_v1.7.0.zip` - 配布用アーカイブ（約440KB）

### その他のバージョン

- `local_llm_chat_v1.6.x/` - 以前のバージョン
- `local_llm_chat_v2.0/` - モジュール分割版（参照用）

## LM Studio バージョン別の違い

| 機能 | 0.3.x | 0.4.x |
|------|-------|-------|
| サーバー起動 | 手動（開発者タブ） | **モデルロード時に自動** |
| 複数モデル同時ロード | 非対応 | **対応** |
| CORS設定場所 | 開発者タブ | **設定 → Local Server** |

> **推奨**: v1.7.0 の全機能を使用するには **LM Studio v0.4.0以降** を使用してください。

## 技術仕様

- **フロントエンド**: HTML5, CSS3, JavaScript (ES6+, Vanilla JS)
- **API**: OpenAI 互換 API（LM Studio、Ollama など）
- **ストレージ**: localStorage (ブラウザ)
- **外部依存**:
  - marked.js (Markdown レンダリング) - ローカル同梱
  - PDF.js (PDF テキスト抽出) - ローカル同梱

## ドキュメント

- [MANUAL.md](local_llm_chat_v1.7.0/MANUAL.md) - ユーザーマニュアル
- [CHANGELOG.md](CHANGELOG.md) - 変更履歴

## バージョン情報

- **現在のバージョン**: 1.7.0
- **リリース日**: 2026-02-01
- **対象**: 2026 JRC ハンズオンセミナー

## ライセンス

MIT License

## サポート

問題が発生した場合は、まず [MANUAL.md](local_llm_chat_v1.7.0/MANUAL.md) のトラブルシューティングセクションを確認してください。

## 謝辞

テスト運用とフィードバックにご協力いただいた皆様に感謝いたします。

- 入江隆介先生
- 守田裕一先生
- 笹山大樹先生

v2.0 の開発にあたり協力いただいた方：
- Ryo Nishizawa (西澤 亮)

---

**Happy Chatting! 🎉**
